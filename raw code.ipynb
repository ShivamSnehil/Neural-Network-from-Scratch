{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ynXQHy_1Ei0K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Activation Functions\n",
        "\n",
        "# Sigmoid function and its derivative for binary tasks\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(a):\n",
        "    return a * (1 - a)\n",
        "\n",
        "# ReLU function and its derivative for hidden layers\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(a):\n",
        "    return (a > 0).astype(float)\n",
        "\n",
        "# Linear activation for regression outputs\n",
        "def linear(z):\n",
        "    return z\n",
        "\n",
        "def linear_derivative(a):\n",
        "    return np.ones_like(a)\n",
        "\n",
        "# Softmax activation for multi-class classification\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # numerical stability\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# Loss Functions\n",
        "\n",
        "# Binary cross-entropy loss for binary classification\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)  # avoid log(0)\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "def binary_cross_entropy_derivative(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1e-7, 1 - y_pred)\n",
        "    return -(y_true / y_pred) + (1 - y_true) / (1 - y_pred)\n",
        "\n",
        "# Mean squared error for regression tasks\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def mean_squared_error_derivative(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true)\n",
        "\n",
        "# Cross-entropy loss for multi-class classification\n",
        "def categorical_cross_entropy(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1e-9, 1 - 1e-9)\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "\n",
        "def categorical_cross_entropy_derivative(y_true, y_pred):\n",
        "    return y_pred - y_true  # softmax + cross-entropy combo derivative\n",
        "\n",
        "# Neural Network Class\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_layers, output_size,\n",
        "                 task='classification', optimizer='sgd', activation='relu'):\n",
        "        self.task = task.lower()\n",
        "        self.optimizer = optimizer.lower()\n",
        "        self.activation_name = activation.lower()\n",
        "        self.hidden_layers = hidden_layers\n",
        "\n",
        "        # Adam optimizer parameters\n",
        "        self.t = 0\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "        # Choose activation functions\n",
        "        self.act = relu if self.activation_name == 'relu' else sigmoid\n",
        "        self.act_deriv = relu_derivative if self.activation_name == 'relu' else sigmoid_derivative\n",
        "\n",
        "        # Choose loss and output activation based on task type\n",
        "        if self.task == 'classification' and output_size == 1:\n",
        "            self.out_act = sigmoid\n",
        "            self.out_act_deriv = sigmoid_derivative\n",
        "            self.loss = binary_cross_entropy\n",
        "            self.loss_deriv = binary_cross_entropy_derivative\n",
        "        elif self.task == 'classification' and output_size > 1:\n",
        "            self.out_act = softmax\n",
        "            self.out_act_deriv = None  # Not needed explicitly\n",
        "            self.loss = categorical_cross_entropy\n",
        "            self.loss_deriv = categorical_cross_entropy_derivative\n",
        "        elif self.task == 'regression':\n",
        "            self.out_act = linear\n",
        "            self.out_act_deriv = linear_derivative\n",
        "            self.loss = mean_squared_error\n",
        "            self.loss_deriv = mean_squared_error_derivative\n",
        "        else:\n",
        "            raise ValueError(\"Invalid task or output configuration.\")\n",
        "\n",
        "        # Initialize layers and parameters\n",
        "        self.layers = [input_size] + hidden_layers + [output_size]\n",
        "        self.L = len(self.layers) - 1\n",
        "\n",
        "        self.W, self.b = {}, {}\n",
        "        self.mW, self.vW, self.mb, self.vb = {}, {}, {}, {}\n",
        "\n",
        "        # Random initialization\n",
        "        for l in range(1, len(self.layers)):\n",
        "            self.W[l] = np.random.randn(self.layers[l-1], self.layers[l]) * 0.01\n",
        "            self.b[l] = np.zeros((1, self.layers[l]))\n",
        "            self.mW[l] = np.zeros_like(self.W[l])\n",
        "            self.vW[l] = np.zeros_like(self.W[l])\n",
        "            self.mb[l] = np.zeros_like(self.b[l])\n",
        "            self.vb[l] = np.zeros_like(self.b[l])\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.Z, self.A = {}, {}\n",
        "        self.A[0] = X\n",
        "        for l in range(1, self.L + 1):\n",
        "            self.Z[l] = np.dot(self.A[l - 1], self.W[l]) + self.b[l]\n",
        "            self.A[l] = self.out_act(self.Z[l]) if l == self.L else self.act(self.Z[l])\n",
        "        return self.A[self.L]\n",
        "\n",
        "    def backward(self, y, learning_rate):\n",
        "        dZ, dW, db = {}, {}, {}\n",
        "\n",
        "        # Output layer derivative\n",
        "        dZ[self.L] = self.loss_deriv(y, self.A[self.L])\n",
        "        dW[self.L] = np.dot(self.A[self.L - 1].T, dZ[self.L])\n",
        "        db[self.L] = dZ[self.L]\n",
        "\n",
        "        # Backpropagation through hidden layers\n",
        "        for l in reversed(range(1, self.L)):\n",
        "            dA = np.dot(dZ[l + 1], self.W[l + 1].T)\n",
        "            dZ[l] = dA * self.act_deriv(self.A[l])\n",
        "            dW[l] = np.dot(self.A[l - 1].T, dZ[l])\n",
        "            db[l] = dZ[l]\n",
        "\n",
        "        # Update parameters using chosen optimizer\n",
        "        if self.optimizer == 'sgd':\n",
        "            self._apply_sgd(dW, db, learning_rate)\n",
        "        elif self.optimizer == 'adam':\n",
        "            self._apply_adam(dW, db, learning_rate)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported optimizer\")\n",
        "\n",
        "    def _apply_sgd(self, dW, db, lr):\n",
        "        for l in range(1, self.L + 1):\n",
        "            self.W[l] -= lr * dW[l]\n",
        "            self.b[l] -= lr * db[l]\n",
        "\n",
        "    def _apply_adam(self, dW, db, lr):\n",
        "        self.t += 1\n",
        "        for l in range(1, self.L + 1):\n",
        "            self.mW[l] = self.beta1 * self.mW[l] + (1 - self.beta1) * dW[l]\n",
        "            self.vW[l] = self.beta2 * self.vW[l] + (1 - self.beta2) * (dW[l] ** 2)\n",
        "            self.mb[l] = self.beta1 * self.mb[l] + (1 - self.beta1) * db[l]\n",
        "            self.vb[l] = self.beta2 * self.vb[l] + (1 - self.beta2) * (db[l] ** 2)\n",
        "\n",
        "            mW_hat = self.mW[l] / (1 - self.beta1 ** self.t)\n",
        "            vW_hat = self.vW[l] / (1 - self.beta2 ** self.t)\n",
        "            mb_hat = self.mb[l] / (1 - self.beta1 ** self.t)\n",
        "            vb_hat = self.vb[l] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            self.W[l] -= lr * mW_hat / (np.sqrt(vW_hat) + self.epsilon)\n",
        "            self.b[l] -= lr * mb_hat / (np.sqrt(vb_hat) + self.epsilon)\n",
        "\n",
        "    def train(self, X, y, epochs=1000, learning_rate=0.01, shuffle=True):\n",
        "        m = X.shape[0]\n",
        "        for epoch in range(epochs):\n",
        "            if shuffle:\n",
        "                idx = np.random.permutation(m)\n",
        "                X, y = X[idx], y[idx]\n",
        "            total_loss = 0\n",
        "            for i in range(m):\n",
        "                x_sample = X[i:i + 1]\n",
        "                y_sample = y[i:i + 1]\n",
        "                self.forward(x_sample)\n",
        "                total_loss += self.loss(y_sample, self.A[self.L])\n",
        "                self.backward(y_sample, learning_rate)\n",
        "            if epoch % 100 == 0 or epoch == epochs - 1:\n",
        "                print(f\"Epoch {epoch}: Avg Loss = {total_loss / m:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        if self.task == 'classification':\n",
        "            if output.shape[1] > 1:\n",
        "                return np.argmax(output, axis=1)  # multiclass\n",
        "            return (output > 0.5).astype(int)    # binary\n",
        "        return output  # regression\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REGRESSION TEST CASE\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate synthetic data: y = 3x + 7 with noise\n",
        "X_reg = np.random.rand(100, 1)\n",
        "y_reg = 3 * X_reg + 7 + np.random.randn(100, 1) * 0.1\n",
        "\n",
        "# Create and train the neural network for regression\n",
        "reg_model = NeuralNetwork(\n",
        "    input_size=1,\n",
        "    hidden_layers=[10, 10],\n",
        "    output_size=1,\n",
        "    task='regression',\n",
        "    optimizer='adam',\n",
        "    activation='relu'\n",
        ")\n",
        "\n",
        "reg_model.train(X_reg, y_reg, epochs=500, learning_rate=0.01)\n",
        "\n",
        "# Predict on training data and print MSE\n",
        "y_pred_reg = reg_model.predict(X_reg)\n",
        "mse = mean_squared_error(y_reg, y_pred_reg)\n",
        "print(\"\\nFinal Regression MSE:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0N6DQsYEw-_",
        "outputId": "ec522cf0-7349-4079-8dc9-2518c11612a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Avg Loss = 31.9167\n",
            "Epoch 100: Avg Loss = 0.0193\n",
            "Epoch 200: Avg Loss = 0.0184\n",
            "Epoch 300: Avg Loss = 0.0193\n",
            "Epoch 400: Avg Loss = 0.0262\n",
            "Epoch 499: Avg Loss = 0.0193\n",
            "\n",
            "Final Regression MSE: 0.012373456191643716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MULTICLASS CLASSIFICATION TEST\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Generate synthetic 3-class data\n",
        "X_clf, y_clf = make_classification(n_samples=300, n_features=4, n_classes=3,\n",
        "                                   n_informative=3, n_redundant=0, random_state=42)\n",
        "y_clf_oh = OneHotEncoder().fit_transform(y_clf.reshape(-1, 1)).toarray()\n",
        "\n",
        "# Create and train the neural network for multiclass classification\n",
        "clf_model = NeuralNetwork(\n",
        "    input_size=4,\n",
        "    hidden_layers=[12, 8],\n",
        "    output_size=3,\n",
        "    task='classification',\n",
        "    optimizer='adam',\n",
        "    activation='relu'\n",
        ")\n",
        "\n",
        "clf_model.train(X_clf, y_clf_oh, epochs=500, learning_rate=0.01)\n",
        "\n",
        "# Predict on training data and calculate accuracy\n",
        "y_pred_clf = clf_model.predict(X_clf)\n",
        "accuracy = np.mean(y_pred_clf == y_clf)\n",
        "print(\"\\nMulticlass Classification Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvSrdCtdFBaE",
        "outputId": "2739f823-094e-4f66-df7c-0550fe4c25fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Avg Loss = 0.9547\n",
            "Epoch 100: Avg Loss = 0.2351\n",
            "Epoch 200: Avg Loss = 0.3130\n",
            "Epoch 300: Avg Loss = 0.2468\n",
            "Epoch 400: Avg Loss = 0.1896\n",
            "Epoch 499: Avg Loss = 0.2903\n",
            "\n",
            "Multiclass Classification Accuracy: 0.9566666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cL6TfYyQFL_E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}